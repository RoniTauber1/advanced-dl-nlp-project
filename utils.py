# -*- coding: utf-8 -*-
"""utils.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/145ONEJy96_VoWn-gL7C24HKiaTQpIL_1
"""

# !pip install -r "/content/drive/My Drive/Colab Notebooks/requirements.txt"

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
from collections import Counter
from sklearn.feature_extraction.text import CountVectorizer
import warnings
warnings.filterwarnings("ignore")

"""# EDA"""

# Basic data sanity: nulls & duplicates
def data_quality_report(df, text_col='OriginalTweet', label_col='Sentiment'):
    """Print simple quality stats: nulls, duplicates."""
    n = len(df)
    null_text = df[text_col].isna().sum()
    null_label = df[label_col].isna().sum()
    dup_text = df[text_col].duplicated().sum()
    print(f"Rows: {n}")
    print(f"Null {text_col}: {null_text}  |  Null {label_col}: {null_label}")
    print(f"Exact-duplicate {text_col}: {dup_text}")

# 3) Basic data sanity: nulls & duplicates
def data_quality_report(df, text_col='OriginalTweet', label_col='Sentiment'):
    """Print simple quality stats: nulls, duplicates."""
    n = len(df)
    null_text = df[text_col].isna().sum()
    null_label = df[label_col].isna().sum()
    dup_text = df[text_col].duplicated().sum()
    print(f"Rows: {n}")
    print(f"Null {text_col}: {null_text}  |  Null {label_col}: {null_label}")
    print(f"Exact-duplicate {text_col}: {dup_text}")

# 4) Label-wise proportions for hashtags / mentions / URLs
def add_meta_flags(df, text_col='OriginalTweet'):
    """Add boolean columns: has_hashtag / has_mention / has_url."""
    x = df.copy()
    s = x[text_col].astype(str)
    x['has_hashtag'] = s.str.contains(r'#\w+', case=False, regex=True)
    x['has_mention'] = s.str.contains(r'@\w+', regex=True)
    x['has_url']     = s.str.contains(r'http[s]?://', regex=True)
    return x

def proportions_by_label(df, label_col='Sentiment'):
    """Show share of rows with hashtags/mentions/urls per label."""
    cols = ['has_hashtag','has_mention','has_url']
    if not set(cols).issubset(df.columns):
        print("Run add_meta_flags(df) first to create indicator columns.")
        return
    out = (df.groupby(label_col)[cols]
             .mean()
             .rename(columns=lambda c: c.replace('has_', 'share_'))
             .reset_index())
    print(out.to_string(index=False))

def ensure_text_length(df: pd.DataFrame, text_col: str = 'OriginalTweet', length_col: str = 'text_length') -> pd.DataFrame:
    """Ensure a length column exists: count whitespace-separated tokens (words)."""
    if length_col not in df.columns:
        df[length_col] = df[text_col].astype(str).apply(lambda s: len(s.split()))
    return df

def plot_label_distribution(df: pd.DataFrame, label_col: str = 'Sentiment') -> None:
    """Simple bar chart of label distribution."""
    counts = df[label_col].value_counts()
    plt.figure(figsize=(7, 5))
    plt.bar(counts.index.astype(str), counts.values)
    plt.title('Label distribution')
    plt.xlabel('Label')
    plt.ylabel('Count')
    plt.xticks(rotation=15)
    plt.tight_layout()
    plt.show()

def plot_length_hist(df):
    filter = 1000
    plt.figure(figsize=(8, 6))
    sns.histplot(df[df['text_length'] < filter], x='text_length',
                 hue='Sentiment', bins=50, kde=True)
    # Add a vertical line at 50
    plt.axvline(x=50, color='red', linestyle='--', label='Max Length Encoding (40)')
    plt.title('Text Length Distribution')
    plt.xlabel('Number of Words')
    plt.ylabel('Frequency')
    # Get unique labels dynamically from the dataframe
    unique_labels = list(df['Sentiment'].unique())
    unique_labels.append('50 words')  # Add the padding/length marker
    plt.legend(title='Legend', labels=unique_labels)
    plt.show()

def plot_length_box_by_label(df, label_col='Sentiment', length_col='text_length'):
    """Boxplot of text length by label."""
    plt.figure(figsize=(8, 5))
    # Using plain matplotlib to keep dependencies aligned
    labels = df[label_col].unique()
    data = [df.loc[df[label_col]==lab, length_col].dropna().astype(int) for lab in labels]
    plt.boxplot(data, labels=labels, showfliers=True)
    plt.title('Text length by label')
    plt.ylabel('Number of words')
    plt.xticks(rotation=15)
    plt.tight_layout()
    plt.show()

def extract_hashtags(text: str):
    """Extract hashtags as lowercase tokens (with '#')."""
    return re.findall(r"#\w+", str(text).lower())

def add_hashtags_column(df: pd.DataFrame, text_col: str = 'OriginalTweet') -> pd.DataFrame:
    """Add a 'hashtags' list column."""
    df = df.copy()
    df['hashtags'] = df[text_col].apply(extract_hashtags)
    return df

def plot_top_hashtags(df: pd.DataFrame, topn: int = 30) -> None:
    """Bar chart of the top-N hashtags."""
    all_ht = [h for hs in df.get('hashtags', []) for h in (hs or [])]
    if not all_ht:
        print("No hashtags found. Run add_hashtags_column first.")
        return
    c = Counter(all_ht).most_common(topn)
    labels, vals = zip(*c)
    plt.figure(figsize=(10, 6))
    plt.bar(labels, vals)
    plt.title(f'Top {topn} hashtags')
    plt.ylabel('Count')
    plt.xticks(rotation=90)
    plt.tight_layout()
    plt.show()

def top_ngrams(series: pd.Series, ngram_range=(1, 1), topn: int = 20) -> pd.DataFrame:
    """Compute top-N ngrams on a text series (expects whitespace tokenization)."""
    vec = CountVectorizer(ngram_range=ngram_range, lowercase=False, tokenizer=lambda s: s.split())
    X = vec.fit_transform(series.fillna("").astype(str))
    vocab = vec.get_feature_names_out()
    counts = np.asarray(X.sum(axis=0)).ravel()
    order = np.argsort(-counts)[:topn]
    return pd.DataFrame({'ngram': vocab[order], 'count': counts[order]})

def top_ngrams_by_label(df: pd.DataFrame, text_col: str, label_col: str, ngram_range=(1, 1), topn: int = 20) -> pd.DataFrame:
    """Compute top-N ngrams per label."""
    rows = []
    for label, g in df.groupby(label_col):
        tdf = top_ngrams(g[text_col], ngram_range=ngram_range, topn=topn)
        tdf['label'] = label
        rows.append(tdf)
    return pd.concat(rows, ignore_index=True)

def plot_top_ngrams(df_ngrams: pd.DataFrame, topn: int = 20, title: str = 'Top n-grams') -> None:
    """Plot n-grams; if 'label' column exists, create small multiples per label."""
    g = df_ngrams.copy()
    if 'label' in g.columns:
        labels = list(g['label'].unique())
        ncols = min(3, len(labels))
        nrows = int(np.ceil(len(labels) / ncols))
        fig, axes = plt.subplots(nrows, ncols, figsize=(6 * ncols, 4 * nrows), squeeze=False)

        for i, lab in enumerate(labels):
            ax = axes[i // ncols, i % ncols]
            gi = g[g['label'] == lab].nlargest(topn, 'count')
            ax.bar(gi['ngram'], gi['count'])
            ax.set_title(f"{title} - {lab}")
            ax.set_xticks(range(len(gi)))
            ax.set_xticklabels(gi['ngram'], rotation=90)

        # delete empty subplots (if any)
        axes_flat = axes.ravel()
        for j in range(len(labels), len(axes_flat)):
            fig.delaxes(axes_flat[j])

        plt.tight_layout()
        plt.show()
    else:
        top = g.nlargest(topn, 'count')
        plt.figure(figsize=(10, 6))
        plt.bar(top['ngram'], top['count'])
        plt.title(title)
        plt.xticks(rotation=90)
        plt.tight_layout()
        plt.show()

def class_imbalance_report(df, label_col):
    # Print class distribution and ratio for imbalance check
    counts = df[label_col].value_counts(normalize=False)
    perc   = df[label_col].value_counts(normalize=True).mul(100).round(2)
    rep = pd.DataFrame({'count': counts, 'percent': perc})
    return rep

"""# Cleaning"""

# - Hashtags: safe segmentation; unify COVID variants only (not 'corona')
# - Emojis: demojize to coarse tags; ASCII emoticons -> tags
# - HTML unescape, URLs/@users -> special tokens
# - Symbols: <money>, <percent>, &->and, '=' removed
# - Keep punctuation; cap !/? runs; unicode-safe cleaning
# - Extras (flags): cap repeated letters/emojis, mask long numbers, remove combining marks

import re, html, unicodedata
from collections import Counter
import pandas as pd

# Optional dependencies (fail-safe)
try:
    import emoji
    HAS_EMOJI = True
except Exception:
    HAS_EMOJI = False

try:
    from wordsegment import load as ws_load, segment as ws_segment
    ws_load()
    HAS_WORDSEGMENT = True
except Exception:
    HAS_WORDSEGMENT = False
    def ws_segment(x): return [x]

# ---------------- Hashtags ----------------
KEEP_HASHTAG_AS_IS = {"brexit"}  # add short/brand tags to avoid splitting

def _canon_covid_tag(tag: str) -> str:
    s = str(tag).lower().lstrip('#')
    s = re.sub(r'[_\-\s]+', '', s)
    s = re.sub(r'covid\d*', 'covid', s)
    s = re.sub(r'sarscov2', 'covid', s)
    s = re.sub(r'^(covid)([a-z0-9]+)$', r'\1 \2', s)
    s = re.sub(r'\b(covid)(\s+\1\b)+', r'\1', s)
    return s.strip()

def _safe_segment(piece: str) -> list:
    p = piece.lower()
    if p == 'covid': return ['covid']
    core = re.sub(r"\d", "", p)
    if len(core) <= 4 or any(ch.isdigit() for ch in p) or (p in KEEP_HASHTAG_AS_IS) or not HAS_WORDSEGMENT:
        return [p]
    try:
        seg = ws_segment(p)
        if not seg or len(seg) > 4 or "".join(seg) != p:
            return [p]
        return seg
    except Exception:
        return [p]

def _segment_hashtag_core(core: str) -> str:
    raw = core.lower()
    if raw in KEEP_HASHTAG_AS_IS or len(re.sub(r"\d", "", raw)) <= 6:
        return _canon_covid_tag(core)
    norm = _canon_covid_tag(core)
    norm = norm.replace('_', ' ')
    norm = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', norm)
    toks = []
    for chunk in norm.split():
        toks.extend(_safe_segment(chunk))
    return " ".join(toks)

def handle_hashtags(text: str, keep_marker: bool = False) -> str:
    def repl(m):
        words = _segment_hashtag_core(m.group(1))
        return " ".join(f"hashtag_{w}" for w in words.split()) if keep_marker else words
    return re.sub(r"#([A-Za-z0-9_]+)", repl, text)

# ---------------- Emojis / emoticons ----------------
_ASCII_EMOTICONS = [
    (r"(:-?\)|=\)|:D|;-?\)|\^_\^)", "<smile>"),
    (r"(:-?\(|:'\(|T_T|;-?_?\()", "<sad>"),
    (r"(:-?/|:/)", "<sad>"),
    (r"(>:\(|>=\()", "<angry>")
]
def map_ascii_emoticons(s: str) -> str:
    for pat, tag in _ASCII_EMOTICONS: s = re.sub(pat, f" {tag} ", s)
    return s

def demojize_to_tags(text: str, compact: bool = True) -> str:
    if not isinstance(text, str): return ""
    s = emoji.demojize(text, language='en') if HAS_EMOJI else text
    if compact and HAS_EMOJI:
        s = re.sub(r":.*(smile|grin|joy|laugh|rofl|beaming).*:", "<smile>", s)
        s = re.sub(r":.*(sad|frown|cry|sob|weary).*:", "<sad>", s)
        s = re.sub(r":.*(angry|rage|pout|steam).*:", "<angry>", s)
        s = re.sub(r":.*(heart|hearts|sparkling_heart|heart_exclamation).*:", "<heart>", s)
        s = re.sub(r":[a-z0-9_]+:", "", s)
    return re.sub(r"\s+", " ", s).strip()

# ---------------- Symbols / quotes / unicode ----------------
def _normalize_quotes(s: str) -> str:
    return (s.replace("’", "'").replace("‘", "'").replace("`", "'")
             .replace("“", '"').replace("”", '"'))

def _fix_replacement_char(s: str) -> str:
    return re.sub(r"(?<=[A-Za-z])(?:\uFFFD|\u0092)(?=[A-Za-z])", "'", s)

_CURRENCY = r"[$€£₪¥]"
def _normalize_symbols(s: str, money_token: str = "<money>") -> str:
    s = re.sub(fr"{_CURRENCY}\s*\d+(?:\.\d+)?", money_token, s)  # amounts -> <money>
    s = re.sub(fr"{_CURRENCY}", money_token, s)                  # lone currency -> <money>
    s = re.sub(r"(\d+(?:\.\d+)?)\s*%", r"\1 <percent>", s)
    s = re.sub(r"%", "<percent>", s)
    s = s.replace("&", " and ")
    s = s.replace("=", " ")
    return s

def _unicode_clean(s: str) -> str:
    s = unicodedata.normalize("NFKC", s)
    s = re.sub(r"[\u0000-\u001F\u007F]", "", s)
    s = re.sub(r"[^\w\s<>\-–—'\"!?\.,:;()\[\]{}@#%^\&*/+\=\~`|…$]", "", s)
    return s

def _remove_combining_marks(s: str) -> str:
    return "".join(ch for ch in s if unicodedata.category(ch) != "Mn")

def _limit_punct_runs(s: str) -> str:
    s = re.sub(r"!{4,}", "!!!", s)
    s = re.sub(r"\?{4,}", "???", s)
    s = re.sub(r"([!?]){4,}", r"\1\1\1", s)
    return s

# ---------------- COVID outside hashtags ----------------
_COVID_PATTERNS = [r"\bcovid[\-\s]?19\b", r"\bsars[\-\s]?cov[\-\s]?2\b", r"\bsarscov2\b"]
def normalize_covid_variants(s: str, covid_token: str = "covid") -> str:
    for pat in _COVID_PATTERNS: s = re.sub(pat, covid_token, s, flags=re.IGNORECASE)
    return s

def _collapse_duplicate_tokens(s: str) -> str:
    s = re.sub(r"\b(?:covid\s+){2,}", "covid ", s)
    s = re.sub(r"(?:<covid>\s+){2,}", "<covid> ", s)
    return s

# ---------------- Extras: repeats / numbers / emojis ----------------
def cap_repeated_letters(s: str, max_repeat: int = 3) -> str:
    return re.sub(r"(.)\1{%d,}" % (max_repeat), r"\1" * max_repeat, s)

def is_emoji_char(ch: str) -> bool:
    if len(ch) != 1: return False
    cat = unicodedata.category(ch)
    if cat == "So": return True
    code = ord(ch)
    return (0x1F300 <= code <= 0x1FAFF) or (0x2600 <= code <= 0x27BF) or (0x1F900 <= code <= 0x1F9FF)

def cap_repeated_emojis(s: str, max_repeat: int = 3) -> str:
    out, prev, count = [], None, 0
    for ch in s:
        if is_emoji_char(ch):
            if ch == prev:
                count += 1
                if count <= max_repeat: out.append(ch)
            else:
                prev, count = ch, 1
                out.append(ch)
        else:
            prev, count = None, 0
            out.append(ch)
    return "".join(out)

def mask_long_numbers(s: str, min_len: int = 6) -> str:
    return re.sub(rf"\b\d{{{min_len},}}\b", "<num>", s)

def _collapse_consecutive_duplicates(s: str) -> str:
    """Collapse any consecutive duplicate tokens: 'x x x' -> 'x'."""
    toks = s.split()
    if not toks:
        return s
    out = [toks[0]]
    for t in toks[1:]:
        if t != out[-1]:
            out.append(t)
    return " ".join(out)

# ---------------- Main ----------------
def preprocess_tweet(
    text: str,
    keep_hashtag_marker: bool = False,
    compact_emojis: bool = True,
    cap_letter_repeats: bool = True,
    cap_emoji_repeats: bool = True,
    mask_long_nums: bool = True,
    remove_combining: bool = True
) -> str:
    if not isinstance(text, str): return ""
    s = html.unescape(text)
    s = re.sub(r'^\s*rt\s+', ' ', s, flags=re.IGNORECASE)
    s = re.sub(r"http[s]?://\S+|www\.\S+", "<url>", s)
    s = re.sub(r"@\w+", "<user>", s)

    s = handle_hashtags(s, keep_marker=keep_hashtag_marker)
    s = normalize_covid_variants(s)

    s = map_ascii_emoticons(s)                  # before '=' handling
    s = demojize_to_tags(s, compact=compact_emojis)

    s = _normalize_quotes(s)
    s = _fix_replacement_char(s)
    s = _normalize_symbols(s, money_token="<money>")

    s = _unicode_clean(s)
    if remove_combining: s = _remove_combining_marks(s)
    if cap_letter_repeats: s = cap_repeated_letters(s, 3)
    if cap_emoji_repeats: s = cap_repeated_emojis(s, 3)
    if mask_long_nums: s = mask_long_numbers(s, 6)

    s = _limit_punct_runs(s)
    s = re.sub(r"\s+", " ", s).strip()
    s = _collapse_duplicate_tokens(s)
    s = _collapse_consecutive_duplicates(s)
    return s

